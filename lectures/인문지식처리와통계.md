# 인문지식처리와통계
### 강의 계획서
![image](https://user-images.githubusercontent.com/61646760/157179687-1790f976-9cf4-4b0c-800f-6a986b615d5a.png)
## 언어분석개론
- **자연어처리(natural language processing, NLP)**
  - 자연어이해(natural language understanding, NLU)
    - DH에서 특히 중요
  - 자연어생성(natural language generation, NLG)
    - DH에서 잠정적으로 중요할 수 있음
- 언어 분석의 핵심은 "찾기"와 "바꾸기"
  - 이순신을 검색하면 충무공은 누락됨
    - `이순신 = 충무공`의 의미 관계를 파악할 수 있어야 함
  - 정규 표현식(regular expression)은 고급진 찾기와 바꾸기
- **N-gram**
  - a contiguous sequence of n items from a given sample of text or speech  
    ![image](https://user-images.githubusercontent.com/61646760/157184572-3dcb4afe-5bb3-49f2-a30d-8808897b2309.png)
  - `예) 행복은 너의 마음에 있다.`
    - 1-gram(unigrams) : “행복은”, “너의”, “마음에”, “있다”
    - 2-gram(bigrams) : “행복은 너의”, “너의 마음에”, “마음에 있다”
    - 3-gram(trigrams) : “행복은 너의 마음에”, “너의 마음에 있다”
  - 교착어로서 조사, 어미의 빈도가 높은 한국어에는 적용하기 어려운 점이 있음
    - 중문을 n 단위로 나누는 경우, 각각의 token이 독립적 의미가 없는 경우가 많음
    - 고전 텍스트 연구에서는 많이 사용됨
- **형태소 분석(morphological analysis)**
  ![image](https://user-images.githubusercontent.com/61646760/157186526-1ac3d255-b01d-456b-9be5-4b9a9556e00c.png)
  - 사전과 어법은 코퍼스(corpus)를 통해 얻음
    - 코퍼스 구축은 까다로운 작업이므로 WPM 고안
    - **Word Piece Model(WPM)**
      - 하나의 단어를 내부 단어(Subword Unit)들로 분리하는 단어 분리 모델
        - sentencepiece
      - 언어 분석의 미래
  - `예) 나는 은이 좋아.`
    - [('나', 'Noun'), ('는', 'Josa'), ('은', 'Noun'), ('이', 'Josa'), ('좋아', 'Adjective'), ('.', 'Punctuation')]
  - 형태소 분석의 문제점 - 한국말은 끝까지
    - `예) 나는 선배 앞에서 멋있는 척 하는 그대를 사랑으로 감싸줄 수 없다.`
      - 부정적인 문장인데 형태소 분석은 긍정으로 판단함
  - **감정 분석(sentiment analysis)**
    - 감정 사전, 형태소 분석기를 통한 텍스트의 감정 분석
    - 문장의 감정 판단은 주관적이라는 비판이 존재
- **공기어 분석(co-occurrence analysis)**
  - 공기어(co-occurrence word) : 같은 문맥 안에서 함께 나타나 서로 밀접한 의미 관계를 갖는 단어
  - 개념사 연구에 중요  
    ![image](https://user-images.githubusercontent.com/61646760/157191227-0cd908d5-41e1-4255-99f2-90a6139224ad.png)
### 인공지능
- 인공지능의 양대 흐름
  - Symbolic (Ontology) : 인간의 지식을 기호로 표시하고, 이를 바탕으로 논리, 검색, 문제 표현 등을 처리
    - **시맨틱 데이터(semantic data)**
      - **RDF(Resource Description Framework)**
      - **온톨로지(ontology)**
      - **LOD(Linked Open Data)** : 정해진 규칙에 따라 구조화한 데이터를 웹상에 발행한 데이터
  - Subsymbolic (Machine Learning) : 지식의 표현 없이 기계 학습과 같은 방법을 이용하여 학습, 패턴 인식과 같은 분야에 활용
    - **딥러닝(deep learning)**
      - 지도 학습(Supervised Learning) : 정해진 답 有
        - `예) seq2seq`
      - 비지도 학습(Unsupervised Learning) : 정해진 답 無
        - `예) Word2Vec`
      - 강화 학습(Reinforcement Learning) : 반복을 통해 목표 달성
        - 인문학에서의 활용 방안은 낮은 편
      - **설명 가능한 AI(XAI, Explainable Artificial Intelligence)**
        - 머신러닝 알고리즘으로 작성된 결과와 출력을 인간인 사용자가 이해하고 이를 신뢰할 수 있도록 해주는 일련의 프로세스와 방법론
- **임베딩(embedding)**
  - 이미지
    - 이미지 → 숫자 → 알고리즘 → 숫자 →이미지
    - 컴퓨터가 보는 이미지 = 숫자의 조합(RGB)
  - 문자
    - 문자 → 숫자 → 알고리즘 → 숫자 → 문자
    - **One-Hot Encoding**
      - 문장이 증가하면 할수록 열(단어)도 많아진다.
      - 무의미한 랜덤 숫자를 배정하는 것이 아니라, 유의미한 숫자를 배정할 수는 없을까? Word2Vec!
    - **Word2Vec**
      - 단어 벡터 간 유의미한 유사도를 계산  
        ![image](https://user-images.githubusercontent.com/61646760/157194789-7c5838df-bb7d-4634-b57a-3c04cf8684ee.png)
          - king-man과 queen-woman 사이의 위치 관계는 의미 관계를 반영함
      - 학습 방식
        - CBOW(Continuous Bag of Words) : 주변에 있는 단어들을 입력으로 중간에 있는 단어들을 예측하는 방법
        - Skip-Gram : 중간에 있는 단어들을 입력으로 주변 단어들을 예측하는 방법
- **seq2seq**
  - 한 문장(시퀀스)을 다른 문장(시퀀스)으로 변환하는 모델
  - `예) 번역기, 챗봇`
  - 인문학에서의 활용도는 낮은 편
- **전이 학습(transfer learning)**
  - 하나의 작업을 위해 훈련된 모델(Pre-training Model)을 유사 작업 수행 모델의 시작점으로 활용하는 딥러닝 접근법
    - `예) Bert / KoBert / KR-BERT /  ERNIE /  gpt-3 / gpt-2 / KoGPT2`
  - **Pre-training Model**
    - 사전에 학습되는 모델
    - 이를 활용하여 새로운 모델을 학습하는 과정은 Fine-tuning
